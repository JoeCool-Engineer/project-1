{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2 (Alternate): SVD Analysis and Component Selection\n",
    "**Due: Should be finished by 2/18/2025**\n",
    "\n",
    "## Overview\n",
    "Building on your covariance analysis from Milestone 1, you'll now implement SVD-based analysis techniques and explore how to select important components. This connects directly to Lessons 11 (Feature Scaling) and 12 (SVD and Covariance).\n",
    "\n",
    "Continue using your chosen dataset from Milestone 1:\n",
    "1. Stock Returns (`stock_returns.csv`)\n",
    "2. Sensor Readings (`sensor_readings.csv`)\n",
    "3. Image Features (`image_features.csv`)\n",
    "\n",
    "## Learning Objectives\n",
    "1. Implement and understand SVD computation\n",
    "2. Analyze explained variance through SVD\n",
    "3. Compare different component selection methods\n",
    "4. Visualize and interpret SVD results\n",
    "\n",
    "## Required Deliverables\n",
    "\n",
    "### 1. Implementation (40%)\n",
    "\n",
    "# SVD Computation\n",
    "The Singular Value Decomposition (SVD) is a fundamental matrix factorization:\n",
    "- Decomposes data matrix X into U, s, and Vh\n",
    "- U: New coordinate system based on data variation\n",
    "- s: Importance of each direction (singular values)\n",
    "- Vh: How original features combine into new directions\n",
    "\n",
    "Key preprocessing choices:\n",
    "- Centering: Remove mean to focus on variation\n",
    "- Scaling: Make features comparable when units differ\n",
    "- Both affect interpretation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_svd(X: np.ndarray, \n",
    "                center: bool = True,\n",
    "                scale: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute SVD of data matrix with optional preprocessing\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix (n_samples, n_features)\n",
    "        center: Whether to center the data\n",
    "        scale: Whether to scale to unit variance\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors (n_samples, n_features)\n",
    "           - Rows are observations\n",
    "           - Columns are new coordinate directions\n",
    "        s: Singular values in descending order\n",
    "           - Square roots of eigenvalues\n",
    "           - Measure importance of each direction\n",
    "        Vh: Right singular vectors (n_features, n_features)\n",
    "           - Rows are principal directions\n",
    "           - Columns are original features\n",
    "    \"\"\"\n",
    "    # TODO: Implement SVD computation\n",
    "    # Hints:\n",
    "    # - Remember centering from Milestone 1\n",
    "    # - Consider when scaling helps/hurts interpretation\n",
    "    # - Look up numpy.linalg.svd full_matrices parameter\n",
    "    # - Think about numerical stability with small values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explained Variance Analysis\n",
    "This function quantifies how much variation each component captures:\n",
    "- Square singular values to get variances\n",
    "- Convert to percentages of total variance\n",
    "- Cumulative sum shows total explained variance\n",
    "- Helps decide how many components to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_explained_variance(s: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Analyze explained variance from singular values\n",
    "    \n",
    "    Args:\n",
    "        s: Array of singular values\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - explained_variance_ratio\n",
    "        - cumulative_variance_ratio\n",
    "    \"\"\"\n",
    "    # TODO: Compute variance ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component Selection\n",
    "This function implements automated dimension selection:\n",
    "- Uses explained variance to choose components\n",
    "- threshold parameter sets minimum variance to retain\n",
    "- Higher threshold keeps more components\n",
    "- Lower threshold gives more aggressive reduction\n",
    "- Balance between complexity and information retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_components(s: np.ndarray,\n",
    "                     threshold: float = 0.95) -> int:\n",
    "    \"\"\"\n",
    "    Select number of components using variance threshold\n",
    "    \n",
    "    Args:\n",
    "        s: Array of singular values\n",
    "        threshold: Minimum cumulative variance to explain\n",
    "        \n",
    "    Returns:\n",
    "        Number of components to keep\n",
    "    \"\"\"\n",
    "    # TODO: Implement component selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD Analysis\n",
    "1. For a simple 2×2 matrix (choose one):\n",
    "   ```\n",
    "   A = [2 1]    or    B = [3 0]    or    C = [1 1]\n",
    "       [1 2]          [0 2]             [1 0]\n",
    "   ```\n",
    "   - Compute SVD by hand showing all steps\n",
    "   - Verify your result multiplying U·Σ·V^T\n",
    "   - Compare with numpy.linalg.svd output\n",
    "\n",
    "2. For your chosen dataset:\n",
    "   - Apply your SVD implementation\n",
    "   - Analyze effect of preprocessing\n",
    "   - Interpret U, s, and Vh matrices\n",
    "\n",
    "#### Component Selection\n",
    "- Implement and compare methods:\n",
    "  * Variance threshold (e.g., keep 90% of variance)\n",
    "  * Elbow method (plot variance vs components)\n",
    "  * Kaiser criterion (keep components with σᵢ > mean(σ))\n",
    "- Justify your recommendations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
