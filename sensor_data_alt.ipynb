{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1 (Alternate): Covariance Estimation and Analysis\n",
    "**Due: Should be finished by 2/11/2025**\n",
    "\n",
    "## Overview\n",
    "In this milestone, you will implement and analyze covariance estimation from data, building directly on the concepts from Lessons 9 (Random Vectors) and 10 (Covariance Matrices). \n",
    "\n",
    "Choose one of the provided datasets:\n",
    "1. Stock Returns (`stock_returns.csv`)\n",
    "2. Sensor Readings (`sensor_readings.csv`)\n",
    "3. Image Features (`image_features.csv`)\n",
    "\n",
    "## Learning Objectives\n",
    "1. Implement and validate covariance estimation from data\n",
    "2. Understand the critical role of centering in covariance estimation\n",
    "3. Visualize and interpret covariance structures\n",
    "4. Analyze how centering affects statistical properties\n",
    "\n",
    "## Required Deliverables\n",
    "\n",
    "### 1. Implementation (40%)\n",
    "\n",
    "#### Covariance Estimation\n",
    "This function implements the core statistical concept of covariance estimation:\n",
    "- Input: Matrix X where each row is an observation and each column is a variable\n",
    "- Output: Square matrix showing relationships between all pairs of variables\n",
    "- Key steps:\n",
    "  1. Center the data (optional but important)\n",
    "  2. Compute pairwise relationships\n",
    "  3. Ensure result is symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_covariance(X: np.ndarray, \n",
    "                       centered: bool = True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute sample covariance matrix from data\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix (n_samples, n_features)\n",
    "        centered: Whether to center the data first (subtract mean)\n",
    "            - True: Compute proper covariance using (X - mean)\n",
    "            - False: Use raw X (not recommended, included for comparison)\n",
    "        \n",
    "    Returns:\n",
    "        Covariance matrix (n_features, n_features)\n",
    "        \n",
    "    Notes:\n",
    "        Centering is crucial for covariance estimation because:\n",
    "        1. Removes mean offset that would bias correlation estimates\n",
    "        2. Makes results interpretable as variance around the mean\n",
    "        3. Ensures positive semidefinite property of result\n",
    "    \"\"\"\n",
    "    # TODO: Implement covariance estimation\n",
    "    # Hints:\n",
    "    # - Review Lesson 10 for covariance matrix definition\n",
    "    # - Look up numpy's mean() function parameters\n",
    "    # - Think about matrix multiplication with transpose\n",
    "    # - Consider how to verify your result is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimator Comparison\n",
    "\n",
    "This function validates your implementation against numpy's trusted version:\n",
    "\n",
    "- Compare your results with numpy.cov()\n",
    "- Check for numerical differences\n",
    "- Understand any discrepancies\n",
    "- Good software engineering practice: test against known good implementation\n",
    "- It's ok if they are not exactly the same, but they should be close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_estimators(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compare your estimator to numpy's implementation\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix\n",
    "        \n",
    "    Returns:\n",
    "        Your covariance matrix, numpy's covariance matrix\n",
    "    \"\"\"\n",
    "    # TODO: Compare your implementation to np.cov()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Size Analysis\n",
    "\n",
    "This function explores how many samples we need for reliable estimates:\n",
    "\n",
    "- Try different sample sizes (small to large)\n",
    "- Sampling means to draw samples from the dataset and compute only on the smaller sample\n",
    "- See how estimates stabilize\n",
    "- Important for real applications where data is limited\n",
    "- Helps understand estimation uncertainty\n",
    "- Advanced (repeatedly sample at the same sample size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sample_size(X: np.ndarray,\n",
    "                       sizes: Optional[np.ndarray] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Analyze how covariance estimate changes with sample size\n",
    "\n",
    "    Args:\n",
    "        X: Full dataset\n",
    "        sizes: Array of sample sizes to test\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with results\n",
    "    \"\"\"\n",
    "    if sizes is None:\n",
    "        sizes = np.logspace(1, np.log10(len(X)), 20).astype(int)\n",
    "    \n",
    "    # TODO: For each size n:\n",
    "    # 1. Sample n points randomly\n",
    "    # 2. Compute covariance\n",
    "    # 3. Track how estimate changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analysis (40%)\n",
    "\n",
    "#### Sample Size Effects\n",
    "\n",
    "- Plot how covariance estimates converge as n increases\n",
    "    - Hint: Plot Frobenius norm of difference between successive estimates\n",
    "        - Frobenius norm is the sum of the squares of the elements of the matrix\n",
    "        - You can use np.linalg.norm(A, ord='fro') to compute the Frobenius norm\n",
    "    - Try logarithmic spacing of sample sizes\n",
    "    - Look for where the curve flattens out\n",
    "- Identify minimum sample size needed for stable estimates\n",
    "    - Look for where changes between successive estimates become small\n",
    "    - Consider setting a threshold (e.g., < 1% change)\n",
    "    - Think about your application's accuracy needs\n",
    "- Compare your results with numpy's implementation\n",
    "    - Use np.allclose() with reasonable tolerance\n",
    "    - Remember: Small numerical differences are normal\n",
    "    - Focus on pattern similarity rather than exact matches\n",
    "\n",
    "#### Visualization and Interpretation\n",
    "\n",
    "- Create covariance heatmaps\n",
    "- Plot confidence ellipses\n",
    "- Interpret the meaning of:\n",
    "    - Diagonal elements\n",
    "    - Off-diagonal elements\n",
    "    - Positive vs negative covariance\n",
    "\n",
    "#### Understanding Data Preprocessing\n",
    "\n",
    "- Centering Analysis:\n",
    "    - Compare covariance with/without centering\n",
    "    - Visualize how centering affects the data cloud\n",
    "    - Prove mathematically why centering is necessary (optional)\n",
    "- Basic Scaling Introduction:\n",
    "    - Scaling means to divide each variable (column) by its standard deviation\n",
    "    - Compare raw vs standardized variables\n",
    "    - Show when different scales cause problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Advanced Topics\n",
    "\n",
    "### Population vs Sample Statistics\n",
    "\n",
    "- Research the difference between population and sample statistics\n",
    "- Investigate why numpy.cov() uses (n-1) denominator by default\n",
    "- Compare with scipy.stats covariance functions\n",
    "- Experiment with different denominators (n vs n-1)\n",
    "- Consider when each might be appropriate\n",
    "\n",
    "Example exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different covariance implementations\n",
    "\n",
    "# Your implementation\n",
    "cov_yours = estimate_covariance(X)\n",
    "\n",
    "# NumPy implementation (uses n-1)\n",
    "cov_numpy = np.cov(X.T)\n",
    "\n",
    "# Manual calculation with n denominator\n",
    "X_centered = X - X.mean(axis=0)\n",
    "cov_pop = (X_centered.T @ X_centered) / len(X)\n",
    "\n",
    "# Compare results and consider:\n",
    "# - When do the differences matter?\n",
    "# - Why might we prefer one over another?\n",
    "# - What assumptions are we making?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
