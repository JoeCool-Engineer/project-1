{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Matrices: From Theory to Practice\n",
    "\n",
    "## 1. Theoretical Foundations\n",
    "\n",
    "\n",
    "## 1.0 Motivation\n",
    "\n",
    "Consider measuring sepal length ($X_1$) and sepal width ($X_2$) for iris flowers in the iris dataset:\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{templates/linalg_010_iris_covariance.png}\n",
    "\\end{figure}\n",
    "\n",
    "1. The red ellipse shows the 95\\% confidence region - assuming the measurements follow a multivariate normal (Gaussian) distribution, this boundary encloses 95\\% of the probability distribution (See **Endnotes**).\n",
    "\n",
    "2. The green arrows show the eigenvectors of the covariance matrix, scaled by their corresponding eigenvalues:\n",
    "   * The longer arrow indicates the direction of maximum variance - the primary axis along which the data varies most\n",
    "   * The shorter arrow shows the direction of minimum variance - there is less variation in this perpendicular direction\n",
    "   * The relative lengths of these arrows (approximately 3.6:1) tell us the data varies about 3.6 times more in the primary direction\n",
    "\n",
    "3. The red dot marks the sample mean $\\boldsymbol{\\mu}$, which serves as the center of the ellipse. The ellipse's:\n",
    "   * Orientation is determined by the eigenvectors\n",
    "   * Shape (eccentricity) is determined by the ratio of eigenvalues\n",
    "   * Size is determined by both eigenvalues and our chosen confidence level (95%)\n",
    "\n",
    "### 1.1 Definition and Basic Properties\n",
    "\n",
    "The population covariance matrix $\\boldsymbol{\\Sigma}$ captures all pairwise relationships:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\boldsymbol{\\Sigma} = E[(\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})^T] = \n",
    "\\begin{bmatrix} \n",
    "\\text{Var}(X_1) & \\text{Cov}(X_1,X_2) \\\\\n",
    "\\text{Cov}(X_2,X_1) & \\text{Var}(X_2)\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "- Each entry $\\sigma_{ij} = E[(X_i-\\mu_i)(X_j-\\mu_j)]$ measures relationship between features $i$ and $j$\n",
    "- Diagonal elements $\\sigma_{ii} = \\text{Var}(X_i)$ capture spread of individual features\n",
    "- Off-diagonal elements $\\sigma_{ij} = \\text{Cov}(X_i,X_j)$ capture feature relationships\n",
    "\n",
    "\\paragraph{For the iris data, this gives:}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\boldsymbol{\\Sigma} \\approx\n",
    "\\begin{bmatrix}\n",
    "0.685 & -0.039 \\\\\n",
    "-0.039 & 0.188\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "\\paragraph{Interpretation:}\n",
    "\n",
    "- Sepal length variance: 0.685 $\\text{cm}^2$\n",
    "- Sepal width variance: 0.188 $\\text{cm}^2$\n",
    "- Slight negative covariance: -0.039 $\\text{cm}^2$\n",
    "\n",
    "\\paragraph{Key properties with proofs:}\n",
    "\n",
    "  * Definition of Positive Semidefiniteness:\n",
    "\n",
    "    A symmetric matrix $\\mathbf{A}$ is positive semidefinite if and only if:\n",
    "    1. $\\mathbf{v}^T\\mathbf{A}\\mathbf{v} \\geq 0$ for all vectors $\\mathbf{v}$ (quadratic form definition)\n",
    "    2. All eigenvalues are non-negative\n",
    "    3. Can be written as $\\mathbf{B}^T\\mathbf{B}$ for some matrix $\\mathbf{B}$\n",
    "    These definitions are equivalent.\n",
    "\n",
    "    In simple terms: A positive semidefinite matrix never makes things \"more negative\" - when you multiply a vector by it, the result always points in a direction that makes a non-negative angle with the original vector. Think of it like a transformation that might squish or stretch things, but never flips them to the opposite side of zero.\n",
    "\n",
    "  * Symmetric: $\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}^T$\n",
    "\n",
    "    *Looking at how variable 1 relates to variable 2 gives you the same information as looking at how variable 2 relates to variable 1*.\n",
    "\n",
    "    - Proof: For any indices $i,j$:\n",
    "      \\begin{align*}\n",
    "      \\sigma_{ij} &= E[(X_i-\\mu_i)(X_j-\\mu_j)] \\\\\n",
    "                  &= E[(X_j-\\mu_j)(X_i-\\mu_i)] \\text{ (scalar multiplication is commutative)} \\\\\n",
    "                  &= \\sigma_{ji}\n",
    "      \\end{align*}\n",
    "  \n",
    "  * Positive semidefinite: $\\mathbf{v}^T\\boldsymbol{\\Sigma}\\mathbf{v} \\geq 0$ for all $\\mathbf{v}$\n",
    "\n",
    "    *When you measure the spread of data in any direction, you always get a non-negative number. You can't have a \"negative spread\"*.\n",
    "\n",
    "    - Proof:\n",
    "      \\begin{align*}\n",
    "      \\mathbf{v}^T\\boldsymbol{\\Sigma}\\mathbf{v} &= \\mathbf{v}^TE[(\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})^T]\\mathbf{v} \\\\\n",
    "      &= E[\\mathbf{v}^T(\\mathbf{X}-\\boldsymbol{\\mu})(\\mathbf{X}-\\boldsymbol{\\mu})^T\\mathbf{v}] \\text{ (linearity of expectation)} \\\\\n",
    "      &= E[(\\mathbf{v}^T(\\mathbf{X}-\\boldsymbol{\\mu}))^2] \\geq 0 \\text{ (square of a real number)}\n",
    "      \\end{align*}\n",
    "  \n",
    "  * Eigenvalues non-negative: $\\lambda_i \\geq 0$\n",
    "\n",
    "    *The eigenvalues tell us how much the data spreads out in different directions. Since spread can't be negative (just like you can't have a negative distance), eigenvalues must always be zero or positive!*.\n",
    "\n",
    "    - Follows from positive semidefiniteness:\n",
    "      If $\\mathbf{u}_i$ is an eigenvector with $\\|\\mathbf{u}_i\\| = 1$, then:\n",
    "      \\begin{align*}\n",
    "      0 \\leq \\mathbf{u}_i^T\\boldsymbol{\\Sigma}\\mathbf{u}_i = \\mathbf{u}_i^T\\lambda_i\\mathbf{u}_i = \\lambda_i\n",
    "      \\end{align*}\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\paragraph{What Does the Covariance Matrix Tell Us?}\n",
    "\n",
    "The covariance matrix is a powerful tool that tells us three key things:\n",
    "\n",
    "1. How much each variable varies on its own:\n",
    "   * The diagonal entries tell us the spread of each variable\n",
    "   * For iris data: sepal length varies more (0.685) than width (0.188)\n",
    "   * Larger values mean more spread in that variable\n",
    "\n",
    "2. How variables move together:\n",
    "   * Off-diagonal entries tell us if variables tend to increase/decrease together\n",
    "   * Positive values: variables tend to increase together\n",
    "   * Negative values: when one goes up, the other tends to go down\n",
    "   * For iris: slight negative relationship (-0.039) between length and width\n",
    "\n",
    "3. What directions matter most:\n",
    "   * Eigenvectors show the important directions in our data\n",
    "   * Eigenvalues tell us how much variation occurs in those directions\n",
    "   * In our plot: the longer green arrow shows the main direction of variation\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "## 2. Linear Algebraic Structure\n",
    "\n",
    "### 2.1 Fundamental Subspaces\n",
    "\n",
    "To visualize these concepts, let's examine the iris petal measurements:\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.45\\textwidth]{templates/linalg_010_iris_petal_covariance.png}\n",
    "\\includegraphics[width=0.45\\textwidth]{templates/linalg_010_iris_petal_rotated.png}\n",
    "\\caption{Left: Original petal measurements with covariance structure. Right: Data rotated to align with principal components (eigenvectors).}\n",
    "\\end{figure}\n",
    "\n",
    "The left plot shows the original petal measurements, while the right plot shows the same data after rotation to align with the eigenvectors. This rotation reveals:\n",
    "\n",
    "1. The row space $\\mathcal{R}(\\boldsymbol{\\Sigma})$:\n",
    "   * Directions of non-zero variance in feature space\n",
    "   * Span of principal components: $\\text{span}\\{\\mathbf{u}_1,\\mathbf{u}_2\\}$\n",
    "   * After rotation, these become the standard basis vectors\n",
    "   * First principal component captures most variation\n",
    "   * Second component is orthogonal, captures remaining variation\n",
    "\n",
    "2. The eigenvalues tell us the scale of variation:\n",
    "   * First component: $\\lambda_1 \\approx 3.116$ (major axis)\n",
    "   * Second component: $\\lambda_2 \\approx 0.039$ (minor axis)\n",
    "   * Ratio $\\lambda_1/\\lambda_2 \\approx 80$ indicates near rank deficiency\n",
    "\n",
    "3. The large eigenvalue ratio reveals an approximate decomposition:\n",
    "   * Row Space: The direction of $\\lambda_1 \\approx 3.116$ captures the true signal\n",
    "     - This single direction explains 98.8% of variation\n",
    "     - Represents the \"effective rank\" of the data\n",
    "     - Suggests data lives primarily on a line\n",
    "   \n",
    "   * \"Approximate\" Null Space: The direction of $\\lambda_2 \\approx 0.039$\n",
    "     - So small it could be treated as effectively zero\n",
    "     - Represents noise or measurement error\n",
    "     - Nearly perfect linear relationship between variables\n",
    "     \n",
    "   This natural separation into dominant and negligible directions:\n",
    "   * Reveals hidden structure in the data\n",
    "   * Suggests dimension reduction is appropriate\n",
    "   * Motivates PCA as a way to find these directions systematically\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "## 3. Statistical Testing of Covariance Structure\n",
    "\n",
    "### 3.1 The F-Distribution and Variance Testing\n",
    "\n",
    "The F-distribution provides a framework for comparing variability between groups. For measurements from two groups:\n",
    "\n",
    "1. Basic F-test:\n",
    "   * Test statistic: $F = \\frac{s_1^2}{s_2^2}$\n",
    "   * Null hypothesis $H_0$: Equal variances ($\\sigma_1^2 = \\sigma_2^2$)\n",
    "   * Alternative $H_1$: Different variances ($\\sigma_1^2 \\neq \\sigma_2^2$)\n",
    "   * Reject $H_0$ if $F < F_{\\alpha/2}$ or $F > F_{1-\\alpha/2}$\n",
    "\n",
    "2. Interpretation:\n",
    "   * $F \\approx 1$: Similar variability\n",
    "   * $F \\gg 1$: First group more variable\n",
    "   * $F \\ll 1$: Second group more variable\n",
    "   * Example: with 49 samples each, reject if $F < 0.61$ or $F > 1.64$\n",
    "\n",
    "### 3.2 Application to Iris Data\n",
    "\n",
    "Let's examine how sepal width varies across species. The F-test compares the variances of sepal width measurements between all three species simultaneously:\n",
    "\n",
    "```python\n",
    "# Results from scipy.stats F-test comparing sepal width variances:\n",
    "# H0: All species have equal variance in sepal width\n",
    "# H1: At least one species has different variance\n",
    "F-statistic: 49.1600\n",
    "p-value: < 0.0001\n",
    "\n",
    "# Sample variances of sepal width:\n",
    "Setosa:     s² = 0.124 cm²\n",
    "Versicolor: s² = 0.098 cm²\n",
    "Virginica:  s² = 0.104 cm²\n",
    "```\n",
    "\n",
    "This F-statistic was computed using scipy.stats.f_oneway, which performs a one-way ANOVA F-test. For variances, this computes:\n",
    "\n",
    "\\begin{equation*}\n",
    "F = \\frac{\\text{variance between groups}}{\\text{variance within groups}} = \\frac{\\sum_{i=1}^k n_i(\\bar{x}_i - \\bar{x})^2/(k-1)}{\\sum_{i=1}^k\\sum_{j=1}^{n_i} (x_{ij} - \\bar{x}_i)^2/(N-k)}\n",
    "\\end{equation*}\n",
    "\n",
    "where:\n",
    "\n",
    "* $k$ = number of groups (3 species)\n",
    "* $n_i$ = number of samples in group $i$\n",
    "* $N$ = total number of samples\n",
    "* $\\bar{x}_i$ = mean of group $i$\n",
    "* $\\bar{x}$ = overall mean\n",
    "\n",
    "This extremely small p-value indicates strong evidence that sepal width variability differs between at least two species. Looking at the sample variances, we can see that Setosa flowers show more variability in sepal width than the other two species.\n",
    "\n",
    "### 3.3 Multivariate Extension\n",
    "\n",
    "For full covariance matrices:\n",
    "\n",
    "1. Test statistic uses determinants:\n",
    "\n",
    "   * $F = \\frac{|\\boldsymbol{\\Sigma}_1|}{|\\boldsymbol{\\Sigma}_2|}$\n",
    "   * $|\\boldsymbol{\\Sigma}| = \\prod_i \\lambda_i$ (product of eigenvalues)\n",
    "   * Represents \"volume\" of variation\n",
    "\n",
    "2. Pairwise Species Comparisons:\n",
    "\n",
    "```python\n",
    "# Covariance determinant ratios and p-values:\n",
    "Setosa vs Versicolor:    F = 0.4220, p = 0.0031\n",
    "Setosa vs Virginica:     F = 0.2408, p < 0.0001\n",
    "Versicolor vs Virginica: F = 0.5706, p = 0.0522\n",
    "```\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "### 3.4 Visual Comparison\n",
    "\n",
    "\\begin{figure}\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{templates/linalg_010_iris_species_comparison.png}\n",
    "\\caption{Comparison of covariance structure across iris species. Each species shows distinct patterns in their sepal measurements, with different means (stars) and 95\\% confidence regions (ellipses).}\n",
    "\\end{figure}\n",
    "\n",
    "The covariance structure varies notably between species:\n",
    "\n",
    "1. Setosa (blue):\n",
    "   * Most compact probability distribution\n",
    "   * Nearly circular confidence region\n",
    "   * Suggests similar variance in both length and width, consistent with spherical normal distribution\n",
    "\n",
    "2. Versicolor (orange):\n",
    "   * Intermediate spread\n",
    "   * Elongated ellipse\n",
    "   * Shows stronger correlation between length and width\n",
    "\n",
    "3. Virginica (green):\n",
    "   * Largest overall spread\n",
    "   * Most elongated confidence region\n",
    "   * Strongest directional trend\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "## 4. Noise Analysis and Statistical Significance\n",
    "\n",
    "### 4.1 Sources of Variance in the Row and Null Spaces\n",
    "\n",
    "Recall that in our petal measurements, we found:\n",
    "\n",
    "* A dominant direction with $\\lambda_1 \\approx 3.116$\n",
    "* A weak direction with $\\lambda_2 \\approx 0.039$\n",
    "\n",
    "This separation helps us understand three fundamental sources of variance:\n",
    "\n",
    "1. Natural Variation (Row Space, $\\lambda_1 \\approx 3.116$):\n",
    "   * True biological differences between specimens\n",
    "   * Environmental effects on growth\n",
    "   * Genetic diversity within species\n",
    "   * Represents genuine biological signal\n",
    "   * Explains 98.8% of total variance\n",
    "\n",
    "2. Measurement Noise (Near \"Null Space\", $\\lambda_2 \\approx 0.039$):\n",
    "   * Instrument precision limits (±0.1mm caliper)\n",
    "   * Human measurement error\n",
    "   * Environmental conditions during measurement\n",
    "   * Comparable to measurement error variance (~0.01 cm²)\n",
    "   * Could be treated as effective zero\n",
    "\n",
    "3. Statistical Noise (Affects Both Spaces):\n",
    "   * Sampling variability\n",
    "   * Finite sample effects\n",
    "   * Estimation uncertainty\n",
    "   * Impacts eigenvalue estimation\n",
    "   * Decreases with sample size\n",
    "\n",
    "### 4.2 Using F-tests to Separate Signal from Noise\n",
    "\n",
    "Our F-test results help distinguish meaningful differences from noise at multiple scales:\n",
    "\n",
    "1. Within Individual Species:\n",
    "   * Setosa width variance: 0.124 cm² (signal + noise)\n",
    "   * Versicolor width variance: 0.098 cm²\n",
    "   * Virginica width variance: 0.104 cm²\n",
    "   * Each significantly above measurement noise (p < 0.0001)\n",
    "\n",
    "2. Within Total Population:\n",
    "   * Overall width variance: 0.188 cm²\n",
    "   * Combines both intra-species variation and between-species differences\n",
    "   * Still significantly above measurement noise (p < 0.0001)\n",
    "   * Larger than any individual species variance\n",
    "\n",
    "3. Between Species Structure:\n",
    "   * Different covariance structures (ellipse shapes)\n",
    "   * F-test confirms these differences are not random\n",
    "   * Suggests distinct biological constraints per species\n",
    "\n",
    "### 4.3 Why Simulation?\n",
    "\n",
    "To validate our understanding of noise effects, we need to:\n",
    "\n",
    "1. Quantify the impact of measurement error\n",
    "2. Test the robustness of our F-statistics\n",
    "3. Verify our \"approximate null space\" interpretation\n",
    "\n",
    "While theoretical analysis helps, simulation offers several advantages:\n",
    "\n",
    "* We can control the noise level precisely\n",
    "* We can repeat experiments many times\n",
    "* We can observe the full distribution of outcomes\n",
    "* We can validate our statistical assumptions\n",
    "\n",
    "### 4.4 Noise Analysis Results\n",
    "\n",
    "To understand how measurement noise affects our analysis, we:\n",
    "\n",
    "1. Started with the original iris measurements\n",
    "2. Added random perturbations (±1mm) to each measurement\n",
    "3. Repeated this process 1000 times\n",
    "4. Computed F-statistics for each perturbed dataset\n",
    "5. Analyzed the distribution of these F-statistics\n",
    "\n",
    "This approach mimics what would happen if we measured the same flowers multiple times with a precision of ±1mm.\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\paragraph{The simulation code:}\n",
    "\n",
    "```python\n",
    "# Parameters\n",
    "n_simulations = 1000\n",
    "measurement_noise = 0.1  # 1mm measurement error\n",
    "\n",
    "# Store results\n",
    "F_stats = []\n",
    "\n",
    "# Simulate noisy measurements\n",
    "for _ in range(n_simulations):\n",
    "    # Add random noise to measurements\n",
    "    X_noisy = X + np.random.normal(0, measurement_noise, X.shape)\n",
    "    \n",
    "    # Compute F-statistic between species\n",
    "    F_stats_sim = []\n",
    "    for i in range(3):\n",
    "        for j in range(i+1, 3):\n",
    "            mask_i = species == i\n",
    "            mask_j = species == j\n",
    "            \n",
    "            # Compute covariance matrices\n",
    "            cov_i = np.cov(X_noisy[mask_i].T)\n",
    "            cov_j = np.cov(X_noisy[mask_j].T)\n",
    "            \n",
    "            # Compute F-statistic\n",
    "            F_stat = np.linalg.det(cov_i) / np.linalg.det(cov_j)\n",
    "            F_stats_sim.append(F_stat)\n",
    "```\n",
    "\n",
    "Using this approach, we simulated 1000 datasets with 1mm measurement error:\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "\\begin{figure}[h]\n",
    "\\centering\n",
    "\\includegraphics[width=0.8\\textwidth]{templates/linalg_010_noise_analysis.png}\n",
    "\\caption{Left: Original iris measurements. Right: Distribution of F-statistics under simulated measurement noise.}\n",
    "\\end{figure}\n",
    "\n",
    "\\paragraph{The simulation reveals:}\n",
    "\n",
    "1. Robustness of F-statistics:\n",
    "   * F-statistics remain significant despite noise because the between-species differences (0.188 cm²) are much larger than our measurement noise (0.01 cm²)\n",
    "   * Species differences persist under measurement error as shown by the clear separation of F-statistic distributions in the right plot\n",
    "   * Natural variation dominates measurement noise - compare the width of the F-statistic distributions (~0.1) to their separation (~0.3)\n",
    "\n",
    "2. Precision Requirements:\n",
    "   * 1mm measurement error (typical caliper precision)\n",
    "   * Natural variation: 2-7mm from the square root of covariance matrix diagonals (√0.188 ≈ 4.3mm average)\n",
    "   * Signal-to-noise ratio = 4.3mm/1mm ≈ 4.3 approaches our threshold of 5 for reliable detection\n",
    "\n",
    "3. Practical Implications:\n",
    "   * Measurement precision is adequate because 1mm error is smaller than the natural variation we observe\n",
    "   * Species differences are robust because F-statistic distributions remain well-separated even with noise\n",
    "   * Current sample sizes (50 per species) provide reliable estimates as shown by the narrow F-statistic distributions\n",
    "\n",
    "\\pagebreak\n",
    "\n",
    "## Endnotes\n",
    "\n",
    "The confidence region is defined mathematically by points $\\mathbf{x}$ satisfying:\n",
    "\\begin{equation*}\n",
    "(\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) = \\chi^2_{2,0.95}\n",
    "\\end{equation*}\n",
    "\n",
    "The value $\\chi^2_{2,0.95} = 5.991$ used in the equation above is the 95th percentile of the chi-square distribution with 2 degrees of freedom. This specific value arises because we're working in 2 dimensions (sepal length and width) and want a 95\\% confidence region.\n",
    "\n",
    "Note on Population vs Sample Covariance:\n",
    "Throughout this visualization, we've used the sample covariance matrix $\\mathbf{S}$ as an estimate of the population covariance matrix $\\boldsymbol{\\Sigma}$. The key differences are:\n",
    "\n",
    "1. Population covariance $\\boldsymbol{\\Sigma}$ uses the true mean $\\boldsymbol{\\mu}$ and divides by $n$:\n",
    "   \\begin{equation*}\n",
    "   \\boldsymbol{\\Sigma} = \\frac{1}{n}\\sum_{i=1}^n (\\mathbf{x}_i-\\boldsymbol{\\mu})(\\mathbf{x}_i-\\boldsymbol{\\mu})^T\n",
    "   \\end{equation*}\n",
    "\n",
    "2. Sample covariance $\\mathbf{S}$ uses the sample mean $\\bar{\\mathbf{x}}$ and divides by $(n-1)$:\n",
    "   \\begin{equation*}\n",
    "   \\mathbf{S} = \\frac{1}{n-1}\\sum_{i=1}^n (\\mathbf{x}_i-\\bar{\\mathbf{x}})(\\mathbf{x}_i-\\bar{\\mathbf{x}})^T\n",
    "   \\end{equation*}\n",
    "\n",
    "The $(n-1)$ denominator in the sample covariance makes it an unbiased estimator of $\\boldsymbol{\\Sigma}$, meaning $E[\\mathbf{S}] = \\boldsymbol{\\Sigma}$. This adjustment accounts for the fact that we're using the estimated mean $\\bar{\\mathbf{x}}$ rather than the true mean $\\boldsymbol{\\mu}$. For large sample sizes, the difference between using $n$ or $(n-1)$ becomes negligible since:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\lim_{n \\to \\infty} \\frac{n-1}{n} = 1\n",
    "\\end{equation*}\n",
    "\n",
    "However, for small samples, using $(n-1)$ is crucial for unbiased estimation. For example, with $n=10$ samples, using $n$ instead of $(n-1)$ would underestimate the variance by about 10%.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
